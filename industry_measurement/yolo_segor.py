# YOLOv5-seg ðŸš€ by Ultralytics, AGPL-3.0 license
import numpy as np
from loguru import logger
import os
import sys
from pathlib import Path
import torch
import cv2
FILE = Path(__file__).resolve()
ROOT = FILE.parents[0] / Path("yolov5_seg")  # YOLOv5 root directory
print(ROOT)
if str(ROOT) not in sys.path:
    sys.path.append(str(ROOT))  # add ROOT to PATH
ROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative
from ultralytics.utils.plotting import Annotator
from yolov5_seg.models.common import DetectMultiBackend
# from utils.dataloaders import  LoadImages
from yolov5_seg.utils.general import (
    Profile,
    cv2,
    scale_boxes,
    non_max_suppression,
)
from yolov5_seg.utils.segment.general import process_mask
from yolov5_seg.utils.torch_utils import select_device
from yolov5_seg.utils.plots import Annotator,colors
from yolov5_seg.utils.augmentations import letterbox


class Predictor():
    def __init__(self,
        weights=ROOT / "yolov5s-seg.pt",  # model.pt path(s)
        imgsz=(640, 640),  # inference size (height, width)
        conf_thres=0.4,  # confidence threshold
        iou_thres=0.3,  # NMS IOU threshold
        device="cpu",  # cuda device, i.e. 0 or 0,1,2,3 or cpu
        line_thickness=1,  # bounding box thickness (pixels)
        half=False,  # use FP16 half-precision inference
        dnn=False,  # use OpenCV DNN for ONNX inference
    ):
    # Load model
        logger.info("LOADING MODEL ------- WAIT")
        self.device = select_device(device)
        self.model = DetectMultiBackend(weights, device=self.device, dnn=dnn, data=None, fp16=half)
        self.names= self.model.names
        self.model.warmup(imgsz=(1 , 3, *imgsz))  # warmup
        self.dt = (Profile(), Profile(), Profile())
        self.line_thickness=line_thickness
        self.conf_thres=conf_thres
        self.iou_thres=iou_thres

    def save_mask(self, img):

        if isinstance(img, str):
            img = cv2.imread(img)

        with self.dt[0]:
            # im = torch.from_numpy(img).to(self.model.device)
            im = torch.from_numpy(img.copy()).permute(2, 0, 1).to(self.model.device)
            im = im.half() if self.model.fp16 else im.float()  # uint8 to fp16/32
            im /= 255  # 0 - 255 to 0.0 - 1.0
            if len(im.shape) == 3:
                im = im[None]  # expand for batch dim

        # Inference
        with self.dt[1]:    # predsåŒ…æ‹¬è¾¹ç•Œæ¡†ã€å¯¹è±¡ç½®ä¿¡åº¦ã€ç±»åˆ«æ¦‚çŽ‡å’Œå®žä¾‹æŽ©ç   # protoä¸ºåŽŸåž‹æŽ©ç 
            pred, proto = self.model(im, augment=False, visualize=False)[:2]

        # NMS
        with self.dt[2]:
            pred = non_max_suppression(pred,self. conf_thres, self.iou_thres,classes= None, agnostic=False, max_det=30, nm=32)

        # Process predictions
        for i, det in enumerate(pred):  # per image
            im0 = img.copy()

            if len(det):
                masks = process_mask(proto[i], det[:, 6:], det[:, :4], im.shape[2:], upsample=True)  # HWC
                det[:, :4] = scale_boxes(im.shape[2:], det[:, :4], im0.shape).round()  # rescale boxes to im0 size

                # show img
                labels = det[:, 5]
                bboxs = det[:, :4]
                bbox_result = []
                # æˆ‘ä»¬çŽ°åœ¨è¦åšçš„ä½¿ç”¨maskä¿¡æ¯ä¿å­˜ä¸€ä¸ªæŽ©ç å›¾åƒï¼Œ ç„¶åŽç”¨è¿™ä¸ªå›¾åƒåŽ»è®¡ç®—å¤–å¾„å’Œå†…å¾„
                masks = masks.cpu()

                masked = img.copy()

                blank_img = np.zeros_like(img)
                for i,bbox in enumerate(bboxs):
                    # x1, y1, x2, y2 = map(int, bbox.tolist())
                    # cv2.rectangle(masked,(x1, y1),(x2, y2),color=[255,0,0],thickness=1)

                    if labels[i]==0:    # wai
                        bbox = [int(x) for x in bbox]
                        bbox_result.append(bbox)
                        blank_img[masks[i] != 0 ] = 255
                        masked[masks[i]!=0]=[255,0,0]
                    # if labels[i]==1:  # nei
                    #     masked[masks[i]!=0]=[0, 0, 255]

                # cv2.imshow("masked", blank_img)
                # cv2.waitKey(0)
                return bbox_result, masked, blank_img
            else:
                print("æœªæ£€æµ‹åˆ°")
                return None, None, None

    def calculate_shim(self, bboxes, mask):
        results = []
        for bbox in bboxes:
            result = {}
            result["Goal_ID"] = 1

            print(bbox)
        return results

def point_on_line(a: np.ndarray, b: np.ndarray, d: float) -> np.ndarray:
        a = np.asarray(a, dtype=float)
        b = np.asarray(b, dtype=float)
        ab = b - a
        len_ab = np.linalg.norm(ab)
        if len_ab == 0:
            raise ValueError("a å’Œ b ä¸èƒ½æ˜¯åŒä¸€ä¸ªç‚¹")
        unit = ab / len_ab          # å•ä½æ–¹å‘å‘é‡
        c = b - d * unit
        return c


if __name__ == "__main__":
    device = "cuda:0" if torch.cuda.is_available() else "cpu"
    weight = "./yolov5_seg/weights/circle-4000.pt"
    predictor = Predictor(weights=Path(weight), device=device)  # 500éžå¸¸å¥½

    img_path = "/home/ubuntu/æ¡Œé¢/dataset1003/snap_20251012_224445_102.png"
    img = cv2.imread(img_path)
    img = letterbox(img, new_shape=(736, 1280), auto=False)[0]
    cv2.imwrite("origin.png", img)
    # ç¬¬ä¸€æ­¥ï¼Œ æ‹¿åˆ°æ¤­åœ†
    bbox, visualized, mask = predictor.save_mask(img) # æ‹¿åˆ°æŽ©ç ï¼Œ mask:(h, w, c)
    
    mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)   # æ‹¿åˆ°æŽ©ç ï¼Œ æŽ¥ä¸‹æ¥åšæ¤­åœ†æ‹Ÿåˆ


    # edge = cv2.Canny(mask, 127, 255)
    # contours, _ = cv2.findContours(edge, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)
    # cnt = max(contours, key=cv2.contourArea)    # æ‹¿åˆ°æœ€å¤§çš„è½®å»“
    # retval = cv2.fitEllipse(cnt)
    # fixed_mask = np.zeros_like(mask)
    # ellipse_mask = np.zeros_like(mask)
    # cv2.ellipse(ellipse_mask, retval, 255, -1)
    # dst = cv2.ellipse(fixed_mask, retval, (255, 255, 255), 2)
    
    # cv2.imwrite("mask.png", ellipse_mask)


    edge = cv2.Canny(mask, 127, 255)
    contours, _ = cv2.findContours(edge, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)
    cnt = max(contours, key=cv2.contourArea)    # æ‹¿åˆ°æœ€å¤§çš„è½®å»“
    retval = cv2.fitEllipse(cnt) # æ¤­åœ†çš„æ—‹è½¬çŸ©å½¢((center_x, center_y), (width, height), angle)
    box = cv2.boxPoints(retval)        # æ¤­åœ†æ—‹è½¬çŸ©å½¢çš„å››ä¸ªé¡¶ç‚¹ï¼Œå·¦ä¸Šï¼Œå³ä¸Šï¼Œå³ä¸‹ï¼Œå·¦ä¸‹ è¿”å›ž 4Ã—2 çš„ numpy æ•°ç»„ï¼Œfloat
    # print("æ—‹è½¬çŸ©å½¢é¡¶ç‚¹:", box)
    print(type(retval))
    print(box)
    points = np.float32(box)
    dstp = np.float32([[0, 0], [640, 0], [640, 640], [0, 640]])
    M = cv2.getPerspectiveTransform(points, dstp)
    cliped_img = cv2.warpPerspective(img, M, (640, 640))
    cv2.imwrite("test.png", cliped_img)

    # ç¬¬äºŒæ­¥ï¼Œ è®¡ç®—æ¤­åœ†çš„ç›¸å…³å‚æ•°ï¼šåœ†å¿ƒï¼Œ é•¿è½´ç«¯ç‚¹ï¼ŒçŸ­è½´ç«¯ç‚¹(æµ®ç‚¹åž‹)
    center, (w, h), angle = retval 

    pt_long1 = (box[1] + box[2])/2
    pt_long2 = (box[0] + box[3])/2
    pt_short1 = (box[0] + box[1])/2
    pt_short2 = (box[2] + box[3])/2

    print("ä¸­å¿ƒï¼š", center)
    print("æ¤­åœ†çš„é•¿ç«¯ç‚¹ï¼š", pt_long1, "çŸ­ç«¯ç‚¹", pt_short1)

    # ç¬¬ä¸‰æ­¥ï¼Œ è®¡ç®—åœ†çš„æ˜ å°„ä¸­å¿ƒï¼Œæˆ‘ä»¬è®¤ä¸ºåç§»è·ç¦»ä¸Žæ¤­åœ†çš„æ‰çŽ‡æœ‰å…³, å¦‚æžœæ˜¯åœ†çš„è¯ï¼Œæ‰çŽ‡ä¸º0
    a = max(retval[1])/2  # é•¿è½´
    b = min(retval[1])/2  # çŸ­è½´
    e = (a - b) / (a + b)
    print("æ‰çŽ‡:", e)
    bias = 50   # åç§»ç³»æ•°ï¼Œéœ€è¦åå¤è°ƒæ•´
    residual = 50 * e 
    c = point_on_line(pt_short1, center, residual)  # åœ†çš„æ˜ å°„ä¸­å¿ƒ
    print("åœ†çš„æ˜ å°„ä¸­å¿ƒ:", c)

    # ç¬¬å››æ­¥ï¼Œ æ ¹æ®åœ†çš„æ˜ å°„ä¸­å¿ƒè®¡ç®—å¤–æŽ¥çŸ©å½¢çš„å››ä¸ªé¡¶ç‚¹
    slope = 0.9 # æ–œçŽ‡ï¼Œä¹Ÿéœ€è¦åå¤è°ƒæ•´ï¼Œç›®å‰è¿˜ä¸æ¸…æ¥šè¯¥æ€Žä¹ˆæŠŠæ–œçŽ‡å’Œåç§»ç³»æ•°è”åˆè°ƒå‚
    from for_ellipse import quad_ADBC_general
    
    A, D, B, C_prime = quad_ADBC_general(center, pt_long1, pt_short1, c, slope)

    # ç¬¬äº”æ­¥ï¼Œ å¯è§†åŒ–ï¼Œ ç”»æ¤­åœ† -> ç”»ä¸­å¿ƒ -> ç”»é€è§†ä¸­å¿ƒ -> ç”»å¤–æŽ¥æ¢¯å½¢
    vis = img
    cv2.ellipse(vis, retval, (0, 255, 0), 2)
    cv2.circle(vis, np.intp(center), 2, (255, 0, 0), 3)
    cv2.circle(vis, np.intp(c), 2, (0, 255, 255), 3)

    A, D, B, Cp = map(lambda p: (int(round(p[0])), int(round(p[1]))), (A, D, B, C_prime))
    cv2.line(vis, A, D, (255, 0, 0), 2)
    cv2.line(vis, B, Cp, (255, 0, 0), 2)
    cv2.line(vis, A, Cp, (255, 0, 0), 2)
    cv2.line(vis, B, D, (255, 0, 0), 2)
    cv2.imshow("img", vis)
    cv2.waitKey(0)
    cv2.destroyAllWindows()

